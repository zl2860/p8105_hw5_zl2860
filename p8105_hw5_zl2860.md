p8105\_hw5\_zl2860
================
Zongchao Liu
11/1/2019

# Load data

``` r
# Load data
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

# Problem 1

``` r
# create a filled-in dataset
fill_in <-
  function(vec){
    if(is.numeric(vec)){
      mean_vec = mean(vec, na.rm= TRUE) # mean is apprxiumated
      vec = replace_na(vec,round(mean_vec, digits = 1))
    }
    else if(is.character(vec)){
      vec = replace_na(vec, "virginica")
    }
  }

data_filled <-
  map_df(iris_with_missing, fill_in)

# check if missing values are replaced
skimr::skim(data_filled)
```

    ## Skim summary statistics
    ##  n obs: 150 
    ##  n variables: 5 
    ## 
    ## ── Variable type:character ────────────────────────────────────────────────────────────────────────────────────────────
    ##  variable missing complete   n min max empty n_unique
    ##   Species       0      150 150   6  10     0        3
    ## 
    ## ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────
    ##      variable missing complete   n mean   sd  p0  p25 p50  p75 p100
    ##  Petal.Length       0      150 150 3.77 1.6  1   1.7  4   4.97  6.9
    ##   Petal.Width       0      150 150 1.19 0.72 0.1 0.4  1.2 1.8   2.5
    ##  Sepal.Length       0      150 150 5.82 0.78 4.3 5.12 5.8 6.38  7.9
    ##   Sepal.Width       0      150 150 3.08 0.4  2   2.82 3.1 3.27  4.4
    ##      hist
    ##  ▇▁▁▆▆▆▃▁
    ##  ▇▁▁▇▃▃▂▂
    ##  ▂▆▃▇▃▃▁▁
    ##  ▁▂▃▇▂▂▁▁

Missing values are replaced.

# Problem 2

## Tidy the data

``` r
file_names = list.files("./data/") # create a list of filenames

study_data <-
  tibble( file_names = file_names) %>%
  bind_cols(map_df(str_c("./data/", file_names), read_csv)) %>%
  separate(file_names, into = c("arm", "id")) # tidy data

data_con <- 
  study_data %>%
  filter(arm == "con")

data_con %>% #only include control arm
  knitr::kable()
```

| arm | id | week\_1 | week\_2 | week\_3 | week\_4 | week\_5 | week\_6 | week\_7 | week\_8 |
| :-- | :- | ------: | ------: | ------: | ------: | ------: | ------: | ------: | ------: |
| con | 01 |    0.20 |  \-1.31 |    0.66 |    1.96 |    0.23 |    1.09 |    0.05 |    1.94 |
| con | 02 |    1.13 |  \-0.88 |    1.07 |    0.17 |  \-0.83 |  \-0.31 |    1.58 |    0.44 |
| con | 03 |    1.77 |    3.11 |    2.22 |    3.26 |    3.31 |    0.89 |    1.88 |    1.01 |
| con | 04 |    1.04 |    3.66 |    1.22 |    2.33 |    1.47 |    2.70 |    1.87 |    1.66 |
| con | 05 |    0.47 |  \-0.58 |  \-0.09 |  \-1.37 |  \-0.32 |  \-2.17 |    0.45 |    0.48 |
| con | 06 |    2.37 |    2.50 |    1.59 |  \-0.16 |    2.08 |    3.07 |    0.78 |    2.35 |
| con | 07 |    0.03 |    1.21 |    1.13 |    0.64 |    0.49 |  \-0.12 |  \-0.07 |    0.46 |
| con | 08 |  \-0.08 |    1.42 |    0.09 |    0.36 |    1.18 |  \-1.16 |    0.33 |  \-0.44 |
| con | 09 |    0.08 |    1.24 |    1.44 |    0.41 |    0.95 |    2.75 |    0.30 |    0.03 |
| con | 10 |    2.14 |    1.15 |    2.52 |    3.44 |    4.26 |    0.97 |    2.73 |  \-0.53 |

## Make a Spaghetti Plot

``` r
study_data %>%
  pivot_longer( week_1 : week_8,
                names_to = "week",
                names_prefix = "week_",
               values_to = "observations") %>%
  mutate(arm_id = str_c(arm,id)) %>%
  ggplot(aes( x = week, y = observations, color = arm, group = arm_id)) +
  geom_line() +
  ggsci::scale_color_jco() +
  theme_bw() +
  labs( title = "A Spaghetti Plot Showing Observations on Each Subject Over Time",
        x = "Week",
        y = "Observations") +
  theme(plot.title = element_text(hjust = .5))
```

![](p8105_hw5_zl2860_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

From the plot above, we see that the average observation on the subjects
in experimental arm is higher than that on the subjects in control arm.
Also, over the 8 weeks, the observations on the subjects in experimental
arm have an increasing trend while the observations in control arm
fluctuate within a range.

# Problem 3

## Set the following design elements

``` r
set.seed(886) # ensure reproducible

# buid function
slr_func <- 
  function(beta_1){
    
    n = 30
    beta_0 = 2
    sigma_squared = 50 #local defalut settings, we only change beta_1
    
    sim_data =  
     tibble(
       x = rnorm(n),
       y = beta_0 + beta_1*x + rnorm(n, mean = 0, sd = sqrt(sigma_squared))) # create raw data for regression
   
   fit = lm(y ~ x, sim_data) # fit model
   
   fit %>%
     broom::tidy() %>%
     select(term,estimate, p.value) %>%
     filter(term == "x") %>%
     mutate(term = recode(term, "x" = "beta_1_hat")) #tidy the results
  } 


# generate 10000 datasets
model_10000 <-
  rerun(10000, slr_func(beta_1 = 0)) %>%
  bind_rows() 

# Repeat the above for β1={1,2,3,4,5,6}
repeated_beta_1 <- 
  tibble(beta_1 = c(1:6)) %>%
  mutate(output_list = map(.x = beta_1, ~rerun(10000, slr_func(beta_1 = .x ))),
         output_dfs = map(output_list,bind_rows)) %>%
  select(-output_list) %>%
  unnest(output_dfs) %>%
  pivot_wider( names_from = term,
               values_from = estimate) %>%
  janitor::clean_names()
```

``` r
repeated_beta_1 %>%
  group_by(beta_1) %>%
  summarize(prop_rejected = sum(p_value < 0.05)/n()) %>%
  ggplot(aes( y = prop_rejected, x = beta_1)) +
  geom_point() +
  geom_smooth() +
  theme_bw() +
  labs( title = "Association Between Effect Size and Power",
        x = "Beta_1",
        y = "Power") +
  theme(plot.title = element_text(hjust = .5))
```

![](p8105_hw5_zl2860_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

From the plot above, we see that as β1 increases, the power
correspondingly increases. however, the power’s rate of increasing is
gradually decreasing as β1
increases.

## Make a plot showing the average estimate of β̂ 1 on the y axis and the true value of β1 on the x axis

``` r
all_avg <-
  repeated_beta_1 %>%
  group_by(beta_1) %>%
  summarize(avg_beta_hat = mean(beta_1_hat))
  

rejected_avg <- 
  repeated_beta_1 %>%
  mutate(rejection = ifelse( p_value < 0.05, "yes", "no")) %>%
  filter( rejection == "yes") %>%
  group_by(beta_1) %>%
  summarize(avg_beta_hat = mean(beta_1_hat))

ggplot() + 
  geom_point(aes(x = beta_1, y = avg_beta_hat), data = all_avg) +
  geom_smooth(aes(x = beta_1, y = avg_beta_hat), data = all_avg) +
  geom_point(aes(x = beta_1, y = avg_beta_hat), data = rejected_avg) +
  geom_smooth(aes(x = beta_1, y = avg_beta_hat, color = "red"), data = rejected_avg) +
  theme_bw() +
  labs( title = "Association Between the Average Estimates and True Values",
        y = "Average Beta1 Hat",
        x = "Beta1") +
  theme(plot.title = element_text(hjust = .5)) +
  ggsci::scale_color_lancet()
```

![](p8105_hw5_zl2860_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

The sample average of β̂1 across tests for which the null is rejected is
always higher than the true value of β1. However, as the true value
increases, the sample average of β̂1 gradually approxiates the true
value.

As we see in the first plot, when the true β1 is small, the power is
also low. Therefore, given the null hypothesis β1 = 0 is FAlSE, the
probability that a sampling result may reject H0 is relatively small.
Hence, a greater “observed β̂1” is needed to reject the FALSE null
hypothesis. However, as the true value of β increases, the power also
increases, which means it becomes easier for an " observed β̂1 " to
reject null hypothesis. That’s why as the true value increases, the
sample average of β̂1 gradually approxiates the true value.
